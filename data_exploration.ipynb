{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7VOLCi4_ika"
   },
   "source": [
    "### Read-In Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DSE4vdBg_ikb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "D2WxgMgh_ike",
    "outputId": "246c1d13-45e9-49a1-de68-aca9a368af43"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/Combined_News_DJIA.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7de93340a609>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcombined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/Combined_News_DJIA.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/RedditNews.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmarket\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/upload_DJIA_table.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/learning_nlp-l6JeSCYk/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/learning_nlp-l6JeSCYk/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/learning_nlp-l6JeSCYk/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/learning_nlp-l6JeSCYk/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/learning_nlp-l6JeSCYk/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/Combined_News_DJIA.csv'"
     ]
    }
   ],
   "source": [
    "combined = pd.read_csv('../data/Combined_News_DJIA.csv')\n",
    "news = pd.read_csv('../data/RedditNews.csv')\n",
    "market = pd.read_csv('../data/upload_DJIA_table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uItA7A90_ikh"
   },
   "outputs": [],
   "source": [
    "print(combined.shape)\n",
    "combined.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cuScVkmo_ikk"
   },
   "outputs": [],
   "source": [
    "# Remove 'b'-prefixes\n",
    "combined = combined.applymap(lambda cell: cell.strip() if type(cell)==str else cell)\n",
    "combined = combined.applymap(lambda cell: cell.lstrip('b\"') if type(cell)==str else cell)\n",
    "combined.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wR6axvPj_ikn"
   },
   "outputs": [],
   "source": [
    "combined.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uy9rp__m_ikq"
   },
   "outputs": [],
   "source": [
    "print(news.shape)\n",
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "813Nvptd_iks"
   },
   "outputs": [],
   "source": [
    "print(market.shape)\n",
    "market.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dMkJrkjB_ikv"
   },
   "source": [
    "### Tokenizer with Vanilla Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Y4arEU7_ikv"
   },
   "outputs": [],
   "source": [
    "news2 = news.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Usqa4Lcd_ikx"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"Parses a string into a list of semantic units (words)\n",
    "\n",
    "    Args:\n",
    "        text (str): The string that the function will tokenize.\n",
    "\n",
    "    Returns:\n",
    "        list: tokens parsed out by the mechanics of your choice\n",
    "    \"\"\"\n",
    "    \n",
    "    tokens = re.sub('[^a-zA-Z 0-9]', '', text)\n",
    "    tokens = tokens.lower().split()\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "69bDhVeC_ikz"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "news2['tokens'] = news2['News'].apply(tokenize)\n",
    "\n",
    "word_counts = Counter()\n",
    "\n",
    "news2['tokens'].apply(lambda x: word_counts.update(x))\n",
    "\n",
    "word_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5l5A-ETb_ik1"
   },
   "outputs": [],
   "source": [
    "# ^ These are stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qUQatccy_ik3"
   },
   "source": [
    "## Tokenization with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xXF5KYHo_ik4"
   },
   "outputs": [],
   "source": [
    "# Sample Corpus from Combined DF\n",
    "# -- Includes Only Top1 Headlines\n",
    "corpus = combined[['Date', 'Label', 'Top1']]\n",
    "print(corpus.shape)\n",
    "corpus.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38O15ORQ_ik6"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load Neural Network\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MyWtNweC_ik8"
   },
   "outputs": [],
   "source": [
    "doc_0 = nlp(corpus['Top1'][0])\n",
    "doc_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VsqBh-sz_ik-"
   },
   "outputs": [],
   "source": [
    "for token in doc_0:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kkNydSOS_ik_"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# spaCy Default Stop-Words\n",
    "default_stop_words = nlp.Defaults.stop_words\n",
    "print(\"Total Default Stop-Words:\\n\", len(default_stop_words))\n",
    "# Check Out Some spaCy Default Stop-Words\n",
    "print(\"7 Random Stop-Words\")\n",
    "set(random.sample(default_stop_words, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xH5bUVBp_ilC"
   },
   "source": [
    "## Removing Stop-Words with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E82HHHYm_ilC"
   },
   "outputs": [],
   "source": [
    "corpus.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNObrfiM_ilE"
   },
   "source": [
    "#### Pipe Functions -> Let Us Loop Through Multiple Documnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "flswN4Tk_ilE"
   },
   "outputs": [],
   "source": [
    "tokens = []\n",
    "\n",
    "\"\"\" Update Tokens w/o Stop-Words \"\"\"\n",
    "# for doc in nlp.pipe(iterable_of_docs, batch_size=500):\n",
    "for doc in nlp.pipe(corpus['Top1'], batch_size=500):\n",
    "    doc_tokens = []\n",
    "    for token in doc:\n",
    "        if (token.is_stop == False) & (token.is_punct == False) & (token.is_space == False):\n",
    "            doc_tokens.append(token.text.lower())\n",
    "        \n",
    "    tokens.append(doc_tokens)\n",
    "    \n",
    "# Create New Tokens Column in Corpus DF     \n",
    "corpus.insert(3, 'tokens', tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5PjUDcg_ilG"
   },
   "outputs": [],
   "source": [
    "# Get Verb Tokens\n",
    "\n",
    "verb_tokens = []\n",
    "\n",
    "\"\"\" Update Tokens w/o Stop-Words \"\"\"\n",
    "for doc in nlp.pipe(corpus['Top1'], batch_size=500):\n",
    "    doc_tokens = []\n",
    "    for token in doc:       \n",
    "        if (token.pos_ == 'VERB'):\n",
    "            doc_tokens.append(token.text.lower())\n",
    "        \n",
    "    verb_tokens.append(doc_tokens)\n",
    "    \n",
    "# Create New Tokens Column in Corpus DF \n",
    "corpus.insert(4, 'verb_tokens', verb_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OWyPJLUO_ilI"
   },
   "outputs": [],
   "source": [
    "corpus['tokens'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q_crhO1x_ilK"
   },
   "outputs": [],
   "source": [
    "corpus['verb_tokens'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0JOzjf_c_ilM"
   },
   "outputs": [],
   "source": [
    "# How we Aggregate All Tokens\n",
    "import itertools\n",
    "aggregate_tokens = list(itertools.chain.from_iterable(corpus['tokens']))\n",
    "aggregate_tokens[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cwxZ0bp0_ilO"
   },
   "outputs": [],
   "source": [
    "# Aggregate Tokens of Corpus\n",
    "print(\"Total Aggregate Tokens:\", len(aggregate_tokens))\n",
    "\n",
    "# Top 10 Non-Stop-Words\n",
    "# WC - Stands for Word Count\n",
    "word_freq = Counter(aggregate_tokens)\n",
    "top_10 = [tup[0] for tup in word_freq.most_common(10)]\n",
    "# This list can be used to identify domain-specific stop words.\n",
    "top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M8vZ_4-l_ilQ"
   },
   "outputs": [],
   "source": [
    "# Aggregate Verb-Tokens of Corpus\n",
    "import itertools\n",
    "aggregate_verb_tokens = list(itertools.chain.from_iterable(corpus['verb_tokens']))\n",
    "print(\"Total Aggregate Verb Tokens:\", len(aggregate_verb_tokens))\n",
    "# Top 10 Verbs\n",
    "# WC - Stands for Word Count\n",
    "verb_freq = Counter(aggregate_verb_tokens)\n",
    "top_10 = [tup[0] for tup in verb_freq.most_common(10)]\n",
    "# This list can be used to identify domain-specific stop words.\n",
    "top_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FrYCIPcU_ilT"
   },
   "source": [
    "### Statistical Trimming -> Common Approach to Stop-Word Removal\n",
    "#### The Idea is Such:\n",
    "1. The words that appear most frequently may not provide any insight into the meaning of the document since they are so prevelant.\n",
    "2. Words that appear infrequently also probably do not add much value, because they are mentioned so rarely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1qoAY8F_ilT"
   },
   "source": [
    "## Stemming & Lemmatization\n",
    "#### This is a form of normalization.\n",
    "Recognizing that killed == kill and batteries == battery in the right context.\n",
    "\n",
    "These words share the same **root** words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHmTbFmQ_ilT"
   },
   "source": [
    "#### Stemming: No-Longer Reccomended for Normalization\n",
    "stemming = a process for removing the commoner morpholical and inflexional endings from words in English (Think: ing, ed, s, ies)\n",
    "- The process of stemming tokens is usually quick because it is **rule** based.\n",
    "- Most stemming is done by well documented algorithms such as Porter, Snowball, and Dawson\n",
    "- Semming might still work well in applications where humans don't have to worry about reading the results. \n",
    "- Search enginenes and more broadly, information retrieval algorithms use stemming because it's so fast. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q8CXGdVW_ilU"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "words = ['wolf', 'wolves']\n",
    "\n",
    "# Stemming is just going to chop off the ends of words and\n",
    "# sometimes create non-words\n",
    "print(\"Example of Stemming Limitations\")\n",
    "for word in words:\n",
    "    print(ps.stem(word))\n",
    "\n",
    "print(\"\\nExample of When Stemming Works\")\n",
    "better_example_words = ['love', 'loves']\n",
    "for word in better_example_words:\n",
    "    print(ps.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SGvXP1am_ilV"
   },
   "source": [
    "### Lemmatization: Higher Computational Cost\n",
    "\n",
    "- More methodical than stemming.\n",
    "- The goal is to transform a word into its base form called a **lemma**.\n",
    "- Plural nouns with funky spellings get transformed to singular tense, verbs are all transformed to the transitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oUUN30vs_ilW"
   },
   "outputs": [],
   "source": [
    "# Small Example of Lemmatization\n",
    "sent = \"This is the start of our NLP adventures. We started here with spaCy. We are starting her with NLP.\"\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "doc = nlp(sent)\n",
    "\n",
    "# Lemma Attributes\n",
    "for token in doc:\n",
    "    print(token.text, \" - \", token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zVHKQB-a_ilY"
   },
   "outputs": [],
   "source": [
    "# Wrap it in a function\n",
    "\n",
    "def get_lemmas(text):\n",
    "    \n",
    "    lemmas = []\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    for token in doc:\n",
    "        if (token.is_stop == False) & (token.is_punct == False) & (token.is_space == False) & (token.pos_ != 'PRON'):\n",
    "            lemmas.append(token.lemma_)\n",
    "            \n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U_CJCqa3_ila"
   },
   "outputs": [],
   "source": [
    "lemmas = corpus['Top1'].apply(get_lemmas)\n",
    "\n",
    "# Create New Lemmas Column in Corpus DF \n",
    "corpus.insert(5, 'lemmas', lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gDMwHjeg_ilc"
   },
   "outputs": [],
   "source": [
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "916-UpIj_ile"
   },
   "outputs": [],
   "source": [
    "corpus.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnT8Qays_ilf"
   },
   "source": [
    "## What are the words accociated with a positive label?\n",
    "\n",
    "Scatter plot of top words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mT4lZPJd_ilf"
   },
   "outputs": [],
   "source": [
    "corpus_a = corpus[['lemmas', 'Label']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8e-7y57K_ilh"
   },
   "outputs": [],
   "source": [
    "# This is a lot, let's break it up. --See Section Below\n",
    "df2 = corpus_a['lemmas'].apply(pd.Series) \\\n",
    "    .merge(corpus_a, right_index=True, left_index = True) \\\n",
    "    .drop('lemmas', axis=1) \\\n",
    "    .melt(id_vars = ['Label'], value_name = 'lemma')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Rows that have ANY missing data\n",
    "df2 = df2.dropna(axis=0)\n",
    "print(df2.shape)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.groupby('lemma').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breaking Down the Above Data Organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK, we start off with corpus_a\n",
    "corpus_a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we make a Series out of each Row/Cell in the Lemmas Column\n",
    "seriesed = corpus_a['lemmas'].apply(pd.Series)\n",
    "# Each 'Cell' is Broken into an Entire Row\n",
    "seriesed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we Merge these Rows of Lemmas BACK with Corups on their indexes.\n",
    "# So essentially we concatenate columns...\n",
    "merged = seriesed.merge(corpus_a, right_index=True, left_index = True)\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we drop the compressed Lemmas column sice we don't\n",
    "# need that information twice\n",
    "dropped = merged.drop('lemmas', axis=1)\n",
    "dropped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where did variable come from?\n",
    "melted = dropped.melt(id_vars = ['Label'], value_name = 'lemma')\n",
    "melted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are values 0-34 for the variable column...\n",
    "# this corresponds to the column integer names for every lemma.\n",
    "melted['variable'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Rows that have ANY missing data\n",
    "dropped_nans = melted.dropna(axis=0)\n",
    "print(dropped_nans.shape)\n",
    "dropped_nans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9S-yhrnQ_iln"
   },
   "outputs": [],
   "source": [
    "# Then we Group Each Lemma by It's Average Label\n",
    "grouped = dropped_nans.groupby('lemma').mean()\n",
    "grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YM2Uf6-_ilq"
   },
   "source": [
    "## The Analysis Continues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at these means with their count in mind...\n",
    "dropped_nans.groupby('lemma').count().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4LbiV55v_ilo"
   },
   "outputs": [],
   "source": [
    "# Obtain Series of Lemma Count\n",
    "count_series = dropped_nans.groupby('lemma').count()['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Count and Label Average Columns\n",
    "lemma_data = grouped.merge(count_series, right_index=True, left_index = True)\n",
    "lemma_data.columns = ['label_avg', 'lemma_count']\n",
    "lemma_data = lemma_data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AZIH1z_O_ilr"
   },
   "outputs": [],
   "source": [
    "# I'm thinking a lot of the 0 and 1 averages are less interesting because\n",
    "# they come from one instance...\n",
    "lemma_data.sort_values('label_avg', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "laG795pN_ilt"
   },
   "outputs": [],
   "source": [
    "# ... Let's test this assumption.\n",
    "count_one_to_one = [False] * len(lemma_data)\n",
    "\n",
    "# I now realize this is a bit redundant ----\n",
    "for i, row in enumerate(lemma_data.iterrows()):\n",
    "    # if lemma_count is 1\n",
    "    if row[1][2] == 1: # ilemma_count\n",
    "        count_one_to_one[i] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Over Half of all Lemmas Fit In This 1-1 Category.\n",
    "sum(count_one_to_one)/len(lemma_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's keep only the lemmas that have more than \n",
    "keepers = ~pd.Series(count_one_to_one)\n",
    "lemma_data_reduced = lemma_data[keepers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now this seems to tell us more about what each lemma might mean\n",
    "# for daily market performance.\n",
    "lemma_data_reduced.sort_values('label_avg', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_one_avg = []\n",
    "\n",
    "for avg in lemma_data_reduced['label_avg']:\n",
    "    if avg == 1.0:\n",
    "        perf_one_avg.append(True)\n",
    "    else:\n",
    "        perf_one_avg.append(False)\n",
    "        \n",
    "print(\"Lemmas that Occur Once:\", sum(perf_one_avg))   \n",
    "print(\"Total Lemmas:\", len(perf_one_avg))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(lemma_data_reduced) == len(perf_one_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_data_reduced.reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We Need to Reset the Index on lemma_data_reduced since...\n",
    "# it starts at 1 for some reason...\n",
    "lemma_data_reduced = lemma_data_reduced.reset_index().drop('index', axis=1)\n",
    "interesting_lemmas = lemma_data_reduced[~pd.Series(perf_one_avg)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_lemmas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Half of all Lemma Counts fall between 2-8\n",
    "# To normalize the comparison between lemmas I will examine\n",
    "# only Lemmas with such a count.\n",
    "interesting_lemmas.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine Histogram without skewed right tail to visualize\n",
    "# distribution of Lemma Counts\n",
    "less_skew = interesting_lemmas[interesting_lemmas['lemma_count'] < 10]\n",
    "less_skew.hist('lemma_count', bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQR Comparison\n",
    "mask = (interesting_lemmas['lemma_count'] >= 2) & \\\n",
    "(interesting_lemmas['lemma_count'] < 8)\n",
    "interesting_lemmas_iqr = interesting_lemmas[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y8I8soqG_ilw"
   },
   "outputs": [],
   "source": [
    "interesting_lemmas_iqr.sort_values('label_avg', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XZN8sDjq_ilx"
   },
   "outputs": [],
   "source": [
    "interesting_lemmas_iqr.plot.scatter('label_avg', 'lemma_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4th Quartile Range Lemmas by Count\n",
    "mask = (interesting_lemmas['lemma_count'] > 8)\n",
    "Q4_lemmas = interesting_lemmas[mask]\n",
    "Q4_lemmas.sort_values('label_avg', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4_lemmas.plot.scatter('lemma_count', 'label_avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_iqr = interesting_lemmas_iqr.sort_values('label_avg', ascending=False)['lemma'].head(10)\n",
    "top_iqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_iqr = interesting_lemmas_iqr.sort_values('label_avg', ascending=False)['lemma'].tail(10)\n",
    "bottom_iqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_Q4 = Q4_lemmas.sort_values('label_avg', ascending=False)['lemma'].head(10)\n",
    "top_Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_Q4 = Q4_lemmas.sort_values('label_avg', ascending=False)['lemma'].tail(10)\n",
    "bottom_Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "data_exploration.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "learning_nlp",
   "language": "python",
   "name": "learning_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
